{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8b4e84b-a787-43a5-9d8f-e9c8285d9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gameselo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math as mt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from porter import stem\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f1c31-4ce1-49d1-8b32-157d94aa1d7b",
   "metadata": {},
   "source": [
    "# DATA COLLECTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8c48fe03-928f-4def-b8dd-b487a5225fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmarco_document = pt.datasets.get_dataset(\"msmarco_document\")\n",
    "msmarcov2_document = pt.datasets.get_dataset(\"msmarcov2_document\")\n",
    "msmarco_passage = pt.datasets.get_dataset(\"msmarco_passage\")\n",
    "msmarcov2_passage = pt.datasets.get_dataset(\"msmarcov2_passage\")\n",
    "vaswani = pt.datasets.get_dataset(\"vaswani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d4e8a693-6a57-4cfe-b4dc-268d6b017ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR MINI TESTS ; OFFICIAL DATASET TO USE IS MSMARCO_DOCUMENT\n",
    "\n",
    "queries_l = vaswani.get_topics()['query'].to_list()\n",
    "id_queries = vaswani.get_topics()['qid'].to_list()\n",
    "\n",
    "queries = dict()\n",
    "i=0\n",
    "for id_q in id_queries:\n",
    "    queries[id_q] = queries_l[i]\n",
    "    i += 1\n",
    "\n",
    "coll = dict()\n",
    "for doc in vaswani.get_corpus_iter():\n",
    "    coll[doc['docno']] = doc['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dd106aa4-149b-4db5-b79f-f148c3626c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement of dielectric constant of liquids by the use of microwave techniques\n"
     ]
    }
   ],
   "source": [
    "print(queries['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6c11311d-8d19-45d8-8e63-a9457471873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compact memories have flexible capacities  a digital data storage\n",
      "system with capacity up to bits and random and or sequential access\n",
      "is described\n"
     ]
    }
   ],
   "source": [
    "print(coll['1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb94100-4638-4dce-ac4b-50cd6869eaba",
   "metadata": {},
   "source": [
    "# RETRIEVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c5341-0999-419b-856d-ca19fc1cb840",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "95a10620-836b-4fb6-84ab-0e769cc6aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token_text):\n",
    "    stops = stopwords.words('english')\n",
    "    new_tokentext = []\n",
    "    for word in token_text:\n",
    "        if word not in stops:\n",
    "            new_tokentext.append(word)\n",
    "    return new_tokentext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b2ec1b40-c931-45b4-8bee-065b255412c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(coll):\n",
    "    for docno in coll.keys():\n",
    "        tmp = remove_stopwords(RegexpTokenizer(r'\\w+').tokenize(coll[docno].lower())) # lower + remove punc + remove stopwords\n",
    "        for j in range(len(tmp)):\n",
    "            tmp[j] = stem(tmp[j])\n",
    "        coll[docno] = tmp\n",
    "        \n",
    "preproc(coll)\n",
    "preproc(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "55083c51-fa06-482e-b1f7-541e6615cd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compact', 'memori', 'flexibl', 'capac', 'digit', 'data', 'storag', 'system', 'capac', 'bit', 'random', 'sequenti', 'access', 'describ']\n",
      "['measur', 'dielectr', 'constant', 'liquid', 'us', 'microwav', 'techniqu']\n"
     ]
    }
   ],
   "source": [
    "print(coll['1'])\n",
    "print(queries['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "23d3d1d2-5469-4193-a0c2-671fbe0503d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_vocab(coll, queries, top_terms=1000):\n",
    "    list_words = []\n",
    "    for docno in coll.keys():\n",
    "        list_words += coll[docno]\n",
    "    \n",
    "    for q_id in queries.keys():\n",
    "        list_words += queries[q_id]\n",
    "            \n",
    "    return dict(Counter(list_words).most_common(top_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "98fcc19a-2bf7-4a2f-aba0-6197c73b3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(top_vocab(coll, queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1169d8e6-2d29-4dfe-a90e-2ad9ac22b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(coll, queries, top_terms=1000):\n",
    "    vocabulary = list(top_vocab(coll, queries, top_terms).keys())\n",
    "    for docno in coll.keys():\n",
    "        tmp = coll[docno]\n",
    "        new_doc = []\n",
    "        for word in tmp:\n",
    "            if word in vocabulary:\n",
    "                new_doc.append(word)\n",
    "        coll[docno] = new_doc\n",
    "        \n",
    "    for q_id in queries.keys():\n",
    "        tmp = queries[q_id]\n",
    "        new_query = []\n",
    "        for word in tmp:\n",
    "            if word in vocabulary:\n",
    "                new_query.append(word)\n",
    "        queries[q_id] = new_query\n",
    "        \n",
    "truncate(coll, queries, top_terms=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "95557869-d53d-41ea-8400-e876140f61d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['memori', 'digit', 'data', 'storag', 'system', 'random', 'describ']\n",
      "['measur', 'dielectr', 'constant', 'liquid', 'us', 'microwav', 'techniqu']\n"
     ]
    }
   ],
   "source": [
    "print(coll['1'])\n",
    "print(queries['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e74ecc10-5684-481d-90d3-b6ca282d9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dict(coll, docno):\n",
    "    tf = dict()\n",
    "    for word in coll[docno]:\n",
    "        if word not in tf.keys():\n",
    "            tf[word] = 1\n",
    "        else:\n",
    "            tf[word] += 1\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4dbb37e9-4de6-4dca-bf30-7da378b477b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_alldocs(coll):\n",
    "    dictTF_alldocs = dict()\n",
    "    for docno in coll.keys():\n",
    "        dictTF_alldocs[docno] = tf_dict(coll, docno)\n",
    "    return dictTF_alldocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "667f4eed-b7cc-4a6f-9316-2474fa08a2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memori': 1, 'digit': 1, 'data': 1, 'storag': 1, 'system': 1, 'random': 1, 'describ': 1}\n"
     ]
    }
   ],
   "source": [
    "dictTF_alldocs = dict_alldocs(coll)\n",
    "print(dictTF_alldocs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "072590ad-02d4-4542-9228-99005237e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_occ(coll, docno):\n",
    "    return len(coll[docno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2b0e7b58-ece5-40ad-9310-d9718f0ec704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(term_occ(coll, '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab8fd1-8d03-4747-acde-80c9ce99a466",
   "metadata": {},
   "source": [
    "## UNEXPANDED RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7610d08d-268d-4180-b67d-74e89c5d3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_ml(coll, term, docno):\n",
    "    if term not in coll[docno]:\n",
    "        return 0\n",
    "    else:\n",
    "        return dictTF_alldocs[docno][term] / term_occ(coll, docno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6c2f1bce-43cc-4c38-a4af-14828879aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_ml_coll(term, coll):\n",
    "    sum_tf = 0\n",
    "    for docno in coll.keys():\n",
    "        if term in coll[docno]:\n",
    "            sum_tf += dictTF_alldocs[docno][term]\n",
    "            \n",
    "    return sum_tf / sum([term_occ(coll, docno) for docno in coll.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2cba6902-9c12-4af5-b9f4-f0a5c68b10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(coll, term, doc, lamb): # P(w|D)\n",
    "    return lamb * p_ml(coll, term, doc) + (1 - lamb) * p_ml_coll(term, coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c1646acf-46b9-43bd-83c4-c7c738d73384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_smoothing(coll, term, doc, mu=1000):\n",
    "    lamb = term_occ(coll, doc) / (term_occ(coll, doc) + mu)\n",
    "    return smoothing(coll, term, doc, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "607919e3-c228-4131-98ef-cb48c3dcb017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_likelihood_retr(coll, query, doc, prob_func): # P(Q|D)\n",
    "    return np.prod(np.array([prob_func(coll,term,doc) for term in query]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bd1e4708-1627-445e-b8ea-d0e897a2d1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.563380651717102e-19\n"
     ]
    }
   ],
   "source": [
    "print(query_likelihood_retr(coll, queries['1'], '1', dirichlet_smoothing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8880a61-743a-4f78-982e-a1d98b115c4c",
   "metadata": {},
   "source": [
    "## EXPANDED RETRIEVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9756c20-2cd0-4358-af02-6e6e55cd3534",
   "metadata": {},
   "source": [
    "### Relevance models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7e1e8c42-bb29-4005-9a9d-8572ba6e3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes(coll, query, doc, prob_func): # returns P(D|Q)\n",
    "    return query_likelihood_retr(coll, query, doc, prob_func) * 1/len(coll) # uniformity for P(D), we assume that we have same probs for all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b1cdc801-7ea8-4685-b3e1-87413ddd5a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3679067737484488e-23\n"
     ]
    }
   ],
   "source": [
    "print(bayes(coll, queries['1'], '1', dirichlet_smoothing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee6d19-667e-4c7a-aec2-2cf5834cd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANKING DOCUMENTS TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c27a6b8-cf9c-4395-8d34-6d8dda51dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMENTS RENVOYÃ‰S PAR LE RETRIEVING = retreived_docs (top 50 docs)\n",
    "# ici coll = top 50 documents SEULEMENT\n",
    "\n",
    "def relevance_model(coll, term, query): # P(w|Q)\n",
    "    return sum([smoothing(coll, term, doc, 0.9) * bayes(coll, query, doc, prob_func) for doc in coll])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a1e146-b4cd-4bf0-b614-b0d55320203c",
   "metadata": {},
   "source": [
    "### Relevance model retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbabdf9e-16f8-4e59-b42d-026db609b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d01761-8aaf-4d70-91da-d54f54f3dc5b",
   "metadata": {},
   "source": [
    "# EXPANSION PREDICTION TASK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423c945-03c5-4ab8-9075-ff5191ff4713",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ba006-d870-42ce-99da-fa9880997014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_P_wQ(vocab, query):\n",
    "    return np.array([relevance_model(w,query) for w in vocab])\n",
    "\n",
    "def P_wColl(term, coll):\n",
    "    return p_ml_coll(term, coll) # P(w|coll) = frequency of term in the entire collection, as well as P_ML(w|coll)\n",
    "\n",
    "def all_P_wcoll(vocab, coll):\n",
    "    return np.array([P_wColl(w, coll) for w in vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1635187-b313-4fc1-acdf-c48eeb67b1a3",
   "metadata": {},
   "source": [
    "## CLARITY METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ef64d-2b73-4bd0-9178-d6b7e3589145",
   "metadata": {},
   "source": [
    "### Weighted clarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de804825-d7a9-4e1d-816a-116e0a383a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def u(w, query, gamma):\n",
    "    if w in query:\n",
    "        return gamma\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc969e4-f4c7-4384-aa1b-14ada4477271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarity(u, coll, query, gamma): # u == func which defines each weight of w\n",
    "    vocabulary = list(top_vocab(coll).keys())\n",
    "    P_WQ = all_P_wQ(vocabulary, query)\n",
    "    P_WColl = all_P_wcoll(vocabulary, query)\n",
    "    u_W = np.array([u(w, query, gamma) for w in vocabulary])\n",
    "    E_AU = np.sum(u_W * P_WQ)\n",
    "    return np.sum((u_W * P_WQ / E_AU) * np.log2(P_WQ / P_WColl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eaff98-0777-477a-8f81-22107f40449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr # Rank correlation\n",
    "\n",
    "gammas = np.logspace(0.1, 1e7, 29)\n",
    "clarities = np.array([np.array([clarity(u, coll, query, gamma) for gamma in gammas]) for query in queries])\n",
    "qldr = np.array([np.array([query_likelihood_retr(query, doc, dirichlet_smoothing) for query in queries]) for doc in coll])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
