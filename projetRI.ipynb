{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b4e84b-a787-43a5-9d8f-e9c8285d9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gameselo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math as mt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.special import entropy # Kullback-Liebler divergence\n",
    "\n",
    "from porter import stem\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb94100-4638-4dce-ac4b-50cd6869eaba",
   "metadata": {},
   "source": [
    "# RETRIEVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c5341-0999-419b-856d-ca19fc1cb840",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a10620-836b-4fb6-84ab-0e769cc6aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token_text):\n",
    "    stops = stopwords.words('english')\n",
    "    new_tokentext = []\n",
    "    for word in token_text:\n",
    "        if word not in stops:\n",
    "            new_tokentext.append(word)\n",
    "    return new_tokentext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec1b40-c931-45b4-8bee-065b255412c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(coll):\n",
    "    for i in range(len(coll)):\n",
    "        tmp = remove_stopwords(RegexpTokenizer(r'\\w+').tokenize(coll[i].lower())) # lower + remove punc + remove stopwords\n",
    "        for j in range(len(tmp)):\n",
    "            tmp[j] = stem(tmp[i])\n",
    "        coll[i] = \" \".join(tmp)\n",
    "        \n",
    "preproc(coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3d1d2-5469-4193-a0c2-671fbe0503d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_vocab(coll, top_terms=1000):\n",
    "    merged_coll = []\n",
    "    for doc in coll:\n",
    "        merged_coll.append(\" \".join(doc))\n",
    "    \n",
    "    merged_coll = \" \".append(merged_coll)\n",
    "            \n",
    "    return dict(Counter(merged_coll).most_common(top_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169d8e6-2d29-4dfe-a90e-2ad9ac22b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_coll(coll, top_terms=1000):\n",
    "    vocabulary = list(top_vocab(coll, top_terms).keys())\n",
    "    for i in range(len(coll)):\n",
    "        tmp = word_tokenize(coll[i])\n",
    "        new_doc = []\n",
    "        for word in tmp:\n",
    "            if word in vocabulary:\n",
    "                new_doc.append(word)\n",
    "        coll[i] = \" \".join(new_doc)\n",
    "        \n",
    "truncate_coll(coll, top_terms=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb37e9-4de6-4dca-bf30-7da378b477b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_alldocs(coll):\n",
    "    dictTF_alldocs = {}\n",
    "    for doc in coll:\n",
    "        dictTF_alldocs[doc] = tf_dict(doc)\n",
    "    return dictTF_alldocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072590ad-02d4-4542-9228-99005237e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_occ(doc):\n",
    "    return sum(list(dictTF_alldocs[doc].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab8fd1-8d03-4747-acde-80c9ce99a466",
   "metadata": {},
   "source": [
    "## UNEXPANDED RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251cded6-5902-422a-88ba-e66c96d9869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_likelihood_retr(query, doc, prob_func): # P(Q|D)\n",
    "    return np.prod(np.array([prob_func(term,doc) for term in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610d08d-268d-4180-b67d-74e89c5d3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_ml(term, doc):\n",
    "    return dictTF_alldocs[doc][term] / term_occ(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f1bce-43cc-4c38-a4af-14828879aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_ml_coll(term, coll):\n",
    "    return sum([dictTF_alldocs[doc][term] for doc in coll]) / sum([term_occ(doc) for doc in coll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba6902-9c12-4af5-b9f4-f0a5c68b10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(term, doc, lamb): # P(w|D)\n",
    "    return lamb * p_ml(term, doc) + (1 - lamb) * p_ml_coll(term, coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1646acf-46b9-43bd-83c4-c7c738d73384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_smoothing(term, doc, mu=1000):\n",
    "    lamb = term_occ(doc) / (term_occ(doc) + mu)\n",
    "    return smoothing(term, doc, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8880a61-743a-4f78-982e-a1d98b115c4c",
   "metadata": {},
   "source": [
    "## EXPANDED RETRIEVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9756c20-2cd0-4358-af02-6e6e55cd3534",
   "metadata": {},
   "source": [
    "### Relevance models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e8c42-bb29-4005-9a9d-8572ba6e3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes(query, doc, prob_func): # returns P(D|Q)\n",
    "    return query_likelihood_retr(query, doc, prob_func) * 1/len(coll) # uniformity for P(D), we assume that we have same probs for all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27a6b8-cf9c-4395-8d34-6d8dda51dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMENTS RENVOYÃ‰S PAR LE RETRIEVING = retreived_docs (top 50 docs)\n",
    "\n",
    "def relevance_model(term, query): # P(w|Q)\n",
    "    return sum([smoothing(term, doc, 0.9) * bayes(query, doc, prob_func) for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a1e146-b4cd-4bf0-b614-b0d55320203c",
   "metadata": {},
   "source": [
    "### Relevance model retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbabdf9e-16f8-4e59-b42d-026db609b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d01761-8aaf-4d70-91da-d54f54f3dc5b",
   "metadata": {},
   "source": [
    "# EXPANSION PREDICTION TASK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423c945-03c5-4ab8-9075-ff5191ff4713",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ba006-d870-42ce-99da-fa9880997014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_P_wQ(vocab, query):\n",
    "    return np.array([relevance_model(w,query) for w in vocab])\n",
    "\n",
    "def all_P_wcoll(vocab, coll):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1635187-b313-4fc1-acdf-c48eeb67b1a3",
   "metadata": {},
   "source": [
    "## CLARITY METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ef64d-2b73-4bd0-9178-d6b7e3589145",
   "metadata": {},
   "source": [
    "### Weighted clarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc969e4-f4c7-4384-aa1b-14ada4477271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarity(p_func, q_func, u, coll, query):\n",
    "    vocabulary = list(top_vocab(coll).keys())\n",
    "    P_WQ = all_P_wQ(vocabulary, query)\n",
    "    u_W = np.array([u])\n",
    "    E_AU = np.sum()\n",
    "    return sum([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
